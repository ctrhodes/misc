{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elemwise{exp,no_inplace}(<TensorType(float64, vector)>)]\n",
      "Looping 1000 times took 18.877000 seconds\n",
      "Result is [ 1.23178032  1.61879341  1.52278065 ...,  2.20771815  2.29967753\n",
      "  1.62323285]\n",
      "Used the cpu\n"
     ]
    }
   ],
   "source": [
    "from theano import function, config, shared, tensor\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core\n",
    "iters = 1000\n",
    "\n",
    "rng = numpy.random.RandomState(22)\n",
    "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))\n",
    "f = function([], tensor.exp(x))\n",
    "print(f.maker.fgraph.toposort())\n",
    "t0 = time.time()\n",
    "for i in range(iters):\n",
    "    r = f()\n",
    "t1 = time.time()\n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\n",
    "print(\"Result is %s\" % (r,))\n",
    "if numpy.any([isinstance(x.op, tensor.Elemwise) and\n",
    "              ('Gpu' not in type(x.op).__name__)\n",
    "              for x in f.maker.fgraph.toposort()]):\n",
    "    print('Used the cpu')\n",
    "else:\n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy import stats, integrate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "import keras\n",
    "#from keras import backend as K\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import Nadam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Chris\\\\Documents\\\\kaggle\\\\personalized medicine\\\\misc-master\\\\misc-master'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is AC1A-A21A\n",
      "\n",
      " Directory of C:\\Users\\Chris\\Documents\\kaggle\\personalized medicine\\misc-master\\misc-master\n",
      "\n",
      "09/30/2017  08:31 PM    <DIR>          .\n",
      "09/30/2017  08:31 PM    <DIR>          ..\n",
      "09/30/2017  08:31 PM    <DIR>          .ipynb_checkpoints\n",
      "09/30/2017  08:31 PM            62,607 full_prec_med_text_vars_API-20170929.ipynb\n",
      "09/30/2017  08:27 PM    <DIR>          out\n",
      "09/30/2017  08:25 PM                 6 README.md\n",
      "09/30/2017  08:25 PM            19,675 stage2_5eps_labels.csv\n",
      "09/30/2017  08:25 PM           144,574 stage2_preds.csv\n",
      "09/30/2017  08:25 PM           152,976 stage2_preds_4epochs.csv\n",
      "09/30/2017  08:25 PM           153,967 stage2_preds_5epochs.csv\n",
      "               6 File(s)        533,805 bytes\n",
      "               4 Dir(s)  423,226,572,800 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST_TEXT_DIR = '/home/rhooads/kaggle/'\n",
    "TEST_VARIANTS_DIR = '/home/rhooads/kaggle/'\n",
    "STAGE1_SOLUTIONS_DIR = '/home/rhooads/kaggle/'\n",
    "\n",
    "TRAINING_TEXT_DIR = '/home/rhooads/kaggle/'\n",
    "TRAINING_VARIANTS_DIR = '/home/rhooads/kaggle/'\n",
    "\n",
    "STAGE2_TEXT_DIR = '/home/rhooads/kaggle/'\n",
    "STAGE2_VARIANTS_DIR = '/home/rhooads/kaggle/'\n",
    "\n",
    "GLOVE_DIR = '/home/rhooads/kaggle/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File /home/rhooads/kaggle/test_text does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mIOError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ffb76d30e617>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTEST_TEXT_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test_text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Chris\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Chris\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Chris\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Chris\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Chris\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1582\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1584\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas\\_libs\\parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas\\_libs\\parsers.c:8873)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File /home/rhooads/kaggle/test_text does not exist"
     ]
    }
   ],
   "source": [
    "test_text = pd.read_csv(os.path.join(TEST_TEXT_DIR, 'test_text'), sep = '\\n')\n",
    "test_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_text = test_text['ID,Text'].str.split('\\|\\|', expand=True, n=1)\n",
    "test_text = test_text.rename(columns={0: \"ID\", 1: \"Text\"})\n",
    "test_text['ID'] = test_text['ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text.tail()\n",
    "#test_text.shape\n",
    "type(test_text['ID'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_variants = pd.read_csv(os.path.join(TEST_VARIANTS_DIR, 'test_variants'))\n",
    "#train_vars = train_vars.sort_index(axis=1)\n",
    "test_variants['Group'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_variants.tail()\n",
    "#test_variants.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_solutions = pd.read_csv(os.path.join(STAGE1_SOLUTIONS_DIR, 'stage1_solution_filtered.csv'))\n",
    "stage1_solutions.head()\n",
    "#stage1_preclasses = stage1_solutions.drop('ID', axis=1).as_matrix()\n",
    "#stage1_preclasses\n",
    "# = pd.DataFrame(stage1_preclasses)\n",
    "#stage1_preclasses_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = stage1_solutions.drop('ID', axis=1)\n",
    "hist = np.argmax(hist.values, axis=1)\n",
    "#test_labels = pd.DataFrame(test_labels, columns = ['Class'])\n",
    "#test_labels['Class'].unique()\n",
    "#test_labels = test_labels.append(pd.DataFrame([2,7,8], columns = ['Class']))\n",
    "#test_labels.head()\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(hist, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text = pd.read_csv(os.path.join(TRAINING_TEXT_DIR, 'training_text'), sep = '\\n')\n",
    "training_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_text = training_text['ID,Text'].str.split('\\|\\|', expand=True, n=1)\n",
    "training_text = training_text.rename(columns={0: \"ID\", 1: \"Text\"})\n",
    "training_text['ID'] = training_text['ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text.head()\n",
    "#training_text.shape\n",
    "type(training_text['ID'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_variants = pd.read_csv(os.path.join(TRAINING_VARIANTS_DIR, 'training_variants'))\n",
    "training_variants['Group'] = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_variants.head()\n",
    "#training_variants.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stage2 here to use variants for one hot encoding\n",
    "stage2_text = pd.read_csv(os.path.join(STAGE2_TEXT_DIR, 'stage2_test_text.csv'), sep = '\\n')\n",
    "stage2_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stage2_text = stage2_text['ID,Text'].str.split('\\|\\|', expand=True, n=1)\n",
    "stage2_text = stage2_text.rename(columns={0: \"ID\", 1: \"Text\"})\n",
    "stage2_text['ID'] = stage2_text['ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_text.tail()\n",
    "stage2_text.shape\n",
    "type(stage2_text['ID'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stage2_variants = pd.read_csv(os.path.join(STAGE2_VARIANTS_DIR, 'stage2_test_variants.csv'))\n",
    "stage2_variants['Group'] = 'stage2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_variants.tail()\n",
    "#test_variants.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants=pd.concat([training_variants.drop('Class', axis=1), test_variants, stage2_variants])\n",
    "variants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_dummies = pd.get_dummies(variants, columns=['Gene', 'Variation'], drop_first=True)\n",
    "variant_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_variants_dummies = variant_dummies[variant_dummies['Group'] == 'test']\n",
    "test_variants_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_no_labels = test_variants_dummies.merge(test_text, left_on='ID', right_on='ID')\n",
    "test_union = pd.merge(test_no_labels, stage1_solutions, on='ID', how='left')\n",
    "test_union.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full = test_union[test_union.isnull().any(axis=1)]\n",
    "test_full = test_full.reset_index(drop=True)\n",
    "test_full = test_full.iloc[:, :test_full.shape[1]-9]\n",
    "test_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_validations = test_union.dropna().copy()\n",
    "test_validations = test_validations.reset_index(drop=True)\n",
    "test_validations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_validations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_variants_dummies = variant_dummies[variant_dummies['Group'] == 'train']\n",
    "training_variants_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_dummies_text = training_variants_dummies.merge(training_text, left_on='ID', right_on='ID')\n",
    "training_dummies_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_variants['Class'] = training_variants['Class']-1\n",
    "training_variants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_one_hot = keras.utils.to_categorical(training_variants['Class'])\n",
    "training_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_one_hot_colnames = pd.DataFrame(training_one_hot, columns=['Class_0', 'Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6', 'Class_7', 'Class_8'])\n",
    "training_one_hot_colnames.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_one_hot_colnames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_full = pd.concat([training_dummies_text, training_one_hot_colnames], axis=1)\n",
    "training_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_full.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_variants_dummies = variant_dummies[variant_dummies['Group'] == 'stage2']\n",
    "stage2_variants_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_full = stage2_variants_dummies.merge(stage2_text, left_on='ID', right_on='ID')\n",
    "stage2_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_full.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_input_text(x):\n",
    "    x['Text'] = x['Text'].str.replace(r'(Go to: [0-9]. )(?!introduction/?|background).*', r'\\1', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'(Go to:) (?!introduction/?|background).*', r'\\1', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace('^.*?Go to: [0-9]. (introduction/?|background)', r'\\1', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace('^.*?Go to: (introduction/?|background)', r'\\1', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'^introduction', '', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'^background', '', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'Go to:', '')\n",
    "    x['Text'] = x['Text'].str.replace('^.*?(Key Words:)', r'\\1', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(' Results.*$| RESULTS .*$', '')\n",
    "    x['Text'] = x['Text'].str.replace('MATERIALS? AND.*$| Methodology.*$', '')\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x80\\x98', '\\'', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x80\\x99', '\\'', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x80\\x9a', ',', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x80\\x9c', '\"', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x80\\x9d', '\"', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\"', '', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x80\\xa2', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x80\\x93', '-', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x80\\x94', '-', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x88\\xbc', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x80\\x8a', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x80\\x82', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x80\\x83', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x82\\xac', '', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xc2\\xae', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x88\\x92', '-', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xc3\\x82', '', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xc2\\xa1', '', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xc3\\xa2', '', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xc2\\xa9', '', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xce\\xb1', 'alpha', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xce\\xb2', 'beta', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xce\\xb3', 'gamma', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xce\\xb4', 'delta', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xce\\xb5', 'epsilon', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xce\\xba', 'kappa', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\t[0-9]{0,3}\\/[0-9]{0,3}', '', flags=re.IGNORECASE) #reference\n",
    "    x['Text'] = x['Text'].str.replace(r'\\t\\[.*?\\]', '', flags=re.IGNORECASE) #reference\n",
    "    x['Text'] = x['Text'].str.replace(r'\\((\\d+, ?)+(\\d+)?\\)', '', flags=re.IGNORECASE) #comma sep ref list in rd brackets\n",
    "    x['Text'] = x['Text'].str.replace(r'\\(\\d+\\)', '', flags=re.IGNORECASE) #single reference in rd brack\n",
    "    x['Text'] = x['Text'].str.replace(r'\\(\\d+-\\d+\\)', '', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\[\\d+\\]', '', flags=re.IGNORECASE) #reference ref in sq brack\n",
    "    x['Text'] = x['Text'].str.replace(r'\\[\\d+-\\d+\\]', '', flags=re.IGNORECASE) #reference x-y in sq brack\n",
    "    x['Text'] = x['Text'].str.replace(r'\\[(\\d+, ?)+(\\d+)?\\]', '', flags=re.IGNORECASE) #comma sep ref list in sq brackets\n",
    "    x['Text'] = x['Text'].str.replace(r'\\(\\w+? et al.*?\\)', '', flags=re.IGNORECASE) #et el.\n",
    "    x['Text'] = x['Text'].str.replace(r'([A-Za-z]{6,}?)[0-9]+?(\\.)', r'\\1\\2', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'([A-Za-z]{6,}?)[0-9]+?(,)', r'\\1\\2', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'([A-Za-z]{6,}?)(\\d+, ?)+(\\d+)?(\\.)', r'\\1\\4', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'(\\d{1,3}?, ?)+(\\d+)?(\\.)', r'\\3', flags=re.IGNORECASE) #open csl\n",
    "    x['Text'] = x['Text'].str.replace(r'\\)(\\d{1,3}?)(\\.)', r'\\2', flags=re.IGNORECASE) #parentheses followed by 1 ref preiod\n",
    "    x['Text'] = x['Text'].str.replace(r'@\\.?EGFR', r'deltaEGFR', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\(to [A-Z]\\. [A-Z]\\.\\)', r'', flags=re.IGNORECASE) #funding abr\n",
    "    x['Text'] = x['Text'].str.replace(r' \\d{4,}\\.? ', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\b[tcga]+\\b', ' ')\n",
    "    x['Text'] = x['Text'].str.replace(r'\\(Fig\\.? ?\\d+ ?[A-Za-z]?.{0,4}\\)', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\(Figure\\.? ?\\d+ ?[A-Za-z]?.{0,4}\\)', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\d{1,2}\\/\\d{1,2}\\/\\d{4}', ' ')\n",
    "    x['Text'] = x['Text'].str.replace(r'\\d{1,2}\\/\\d{1,2}\\/\\d{2}', ' ')\n",
    "    x['Text'] = x['Text'].str.replace(r'((\\(\\d{3}\\) ?)|(\\d{3}-))?\\d{3}-\\d{4}', ' ', flags=re.IGNORECASE) #phone\n",
    "    x['Text'] = x['Text'].str.replace(r'\\(?(http|ftp|https):\\/\\/[\\w\\-_]+(\\.[\\w\\-_]+)+\\/?\\)?', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\(?www\\.[\\w\\-_]+(\\.[\\w\\-_]+)+\\/?\\)?', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'https?:\\/\\/(dx\\.)?doi\\.org\\/.*? ', '', flags=re.IGNORECASE) #http://dx?doi.org\n",
    "    x['Text'] = x['Text'].str.replace(r'doi: ?\\d+\\.\\d+\\/.*? ', '', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'Dept\\.?|Department of \\w+', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'Phone:', r' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'Fax:', r' ', flags=re.IGNORECASE)\n",
    "    return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt_train = process_input_text(training_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_train[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(tt_train.shape[0]):\n",
    "    tt_train.loc[i,'Text'] = tt_train.loc[i,'Text'].decode(\"utf-8\").encode(\"ascii\",\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt_test = process_input_text(test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(tt_test.shape[0]):\n",
    "    tt_test.loc[i,'Text'] = tt_test.loc[i,'Text'].decode(\"utf-8\").encode(\"ascii\",\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt_test_valids = process_input_text(test_validations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_test_valids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(tt_test_valids.shape[0]):\n",
    "    tt_test_valids.loc[i,'Text'] = tt_test_valids.loc[i,'Text'].decode(\"utf-8\").encode(\"ascii\",\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt_stage2 = process_input_text(stage2_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_stage2[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_stage2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(tt_stage2.shape[0]):\n",
    "    tt_stage2.loc[i,'Text'] = tt_stage2.loc[i,'Text'].decode(\"utf-8\").encode(\"ascii\",\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_stage2['Text'][150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tt_test['Text'].str.contains('\\\\\\\\x')\n",
    "tt_test.loc[150,'Text']\n",
    "#asciidata\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_test.loc[741]\n",
    "tt_test.loc[741, 'Text'][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#does entry lack \"reintroduction\n",
    "#reint_mask = ~tt['Text'].str.contains('reintroduction', flags=re.IGNORECASE)\n",
    "#reint_mask[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#theint_mask = ~tt['Text'].str.contains('the introduction', flags=re.IGNORECASE)\n",
    "#theint_mask[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#anint_mask = ~tt['Text'].str.contains('an introduction', flags=re.IGNORECASE)\n",
    "#anint_mask[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#theint_mask[742]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#leng = int(reint_mask.shape[0])\n",
    "#for i in range(leng):\n",
    "#    if (reint_mask[i] == True or theint_mask[i] == True or anint_mask[i] == True):\n",
    "#        tt['Text'][i] = re.sub('Introduction.{,20}?\\\\xe2\\\\x80\\\\xa2.*$', '', tt['Text'][i], flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#leng = int(reint_mask.shape[0])\n",
    "#for i in range(leng):\n",
    "#    if (reint_mask[i] == True or theint_mask[i] == True or anint_mask[i] == True):\n",
    "#        tt['Text'][i] = re.sub('^.*?Introduction', '', tt['Text'][i], flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tt.where(df_mask, tt['Text'].str.replace('Introduction.{,20}?\\\\xe2\\\\x80\\\\xa2.*$', ''))\n",
    "#tt['Text'].head()\n",
    "#tt['Text'] = tt['Text'].str.replace('Introduction.{,20}?\\\\xe2\\\\x80\\\\xa2.*$', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4808 characters - must convert to words for vocab length\n",
    "len(tt_test.loc[741, 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_test.loc[3227]\n",
    "tt_test.loc[3227, 'Text'][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_test.loc[923, 'Text']\n",
    "#tt_test.loc[923, 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_test.loc[150]\n",
    "tt_test.loc[150, 'Text']\n",
    "#1613\n",
    "#529\n",
    "#150\n",
    "#2364\n",
    "#2395"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(tt_stage2['Text'].str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "###########################\n",
    "#RE test here\n",
    "#\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_stage2['Text'].str.len().sort_values(ascending = True)[:20]\n",
    "#[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = pd.DataFrame(tt_test['Text'].str.len())\n",
    "#lens.head()\n",
    "#lens.idxmax()\n",
    "lens.median()\n",
    "#lens.quantile(q=0.75)\n",
    "#If needed, just take the first 5000 words in each entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_corpus = tt_train['Text'].values.tolist()\n",
    "test_corpus = tt_test['Text'].values.tolist()\n",
    "stage2_corpus = tt_stage2['Text'].values.tolist()\n",
    "train_test_stage2_corpus = train_corpus + test_corpus + stage2_corpus\n",
    "\n",
    "test_val_corpus = tt_test_valids['Text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_stage2_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IN GENERAL, DONT MAKE OWN VOCAB\n",
    "#DOWNLAOD GLOVE OR WORD2VEC\n",
    "SEQ_LEN = 2000\n",
    "MAX_WORDS = 5000\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "\n",
    "#or take all words as tokens\n",
    "#tokenizermax_words = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probably have to use a loop for this\n",
    "tokenizer.fit_on_texts(train_test_stage2_corpus)\n",
    "WORD_INDEX = tokenizer.word_index\n",
    "print(WORD_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted(WORD_INDEX.items(), key=operator.itemgetter(1))\n",
    "#sorted(WORD_INDEX.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Found %s unique tokens.' % len(WORD_INDEX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_seq = tokenizer.texts_to_sequences(train_corpus)\n",
    "test_seq = tokenizer.texts_to_sequences(test_corpus)\n",
    "test_val_seq = tokenizer.texts_to_sequences(test_val_corpus)\n",
    "stage2_seq = tokenizer.texts_to_sequences(stage2_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_val_seq[0] #1277:1279"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def limit_words(x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i][:SEQ_LEN]\n",
    "    return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_seq_limit = limit_words(train_seq)\n",
    "test_seq_limit = limit_words(test_seq)\n",
    "test_val_seq_limit = limit_words(test_val_seq)\n",
    "stage2_seq_limit = limit_words(stage2_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_seq[1278])\n",
    "len(test_seq_limit[1278])\n",
    "#test_seq_limit[1278]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pad_sequences(train_seq_limit, maxlen=SEQ_LEN, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pad_sequences(test_seq_limit, maxlen=SEQ_LEN, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_val_data = pad_sequences(test_val_seq, maxlen=SEQ_LEN, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stage2_data = pad_sequences(stage2_seq_limit, maxlen=SEQ_LEN, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(WORD_INDEX) + 1, EMBEDDING_DIM))\n",
    "for word, i in WORD_INDEX.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################3\n",
    "#######################################333\n",
    "########################################3\n",
    "#######################################\n",
    "\n",
    "#Need to combine training_full and test full genes and vars before one hot encoding - for mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Come back to this if needed\n",
    "#pre_MLP_CNN_variants = pd.concat([train_no_text, pd.DataFrame(train_data), pd.DataFrame(train_labels)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchfor = ['ID', 'Gene', 'Variation']\n",
    "searchfor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_text = training_full.loc[:,training_full.columns.str.contains('|'.join(searchfor))]\n",
    "train_no_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = training_full.loc[:,training_full.columns.str.contains('Class')]\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MLP_CNN = pd.concat([train_no_text, pd.DataFrame(train_data), train_labels], axis=1)\n",
    "train_MLP_CNN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_split = .iloc[:,train_no_text.shape[1]:]\n",
    "#x_split = train_MLP_CNN.drop(['Class'], axis=1)\n",
    "x_split = train_MLP_CNN.iloc[:,:(train_MLP_CNN.shape[1]-9)]\n",
    "y_split = train_MLP_CNN.iloc[:,(train_MLP_CNN.shape[1]-9):]\n",
    "x_split.head()\n",
    "#y_split.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_val_no_text = test_validations.loc[:,test_validations.columns.str.contains('|'.join(searchfor))]\n",
    "test_val_no_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_val_no_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vals_labels = test_validations.iloc[:,(test_validations.shape[1]-9):]\n",
    "test_vals_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_MLP_CNN = pd.concat([test_val_no_text, pd.DataFrame(test_val_data), test_vals_labels], axis=1)\n",
    "val_MLP_CNN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_MLP_CNN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validation = val_MLP_CNN.iloc[:,:(val_MLP_CNN.shape[1]-9)]\n",
    "y_validation = val_MLP_CNN.iloc[:,(val_MLP_CNN.shape[1]-9):]\n",
    "x_validation.head()\n",
    "#y_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full_no_text = test_full.loc[:,test_full.columns.str.contains('|'.join(searchfor))]\n",
    "test_full_no_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full_no_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_MLP_CNN = pd.concat([test_full_no_text, pd.DataFrame(test_data)], axis=1)\n",
    "test_MLP_CNN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_MLP_CNN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_no_text = stage2_full.loc[:,stage2_full.columns.str.contains('|'.join(searchfor))]\n",
    "stage2_no_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_no_text.head()\n",
    "stage2_no_text.shape[1]\n",
    "stage2_no_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_MLP_CNN = pd.concat([stage2_no_text, pd.DataFrame(stage2_data)], axis=1)\n",
    "val_MLP_CNN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_split, y_split, stratify=y_split, train_size=0.90, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = pd.concat([x_train, x_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_val = pd.concat([x_val, x_validation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = pd.concat([y_train, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = y_train.columns.values.tolist()\n",
    "cols\n",
    "#y_train[cols] = y_train[cols].applymap(np.int32)\n",
    "y_train[cols] = y_train[cols].astype(int)\n",
    "y_train = y_train.values\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validation = y_validation.rename(columns={\"class1\": \"Class_0\", \"class2\": \"Class_1\", \n",
    "                                            \"class3\": \"Class_2\", \"class4\": \"Class_3\", \n",
    "                                            \"class5\": \"Class_4\", \"class6\": \"Class_5\", \n",
    "                                            \"class7\": \"Class_6\", \"class8\": \"Class_7\", \n",
    "                                            \"class9\": \"Class_8\"})\n",
    "y_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_val = pd.concat([y_val, y_validation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cols = y_val.columns.values.tolist()\n",
    "val_cols\n",
    "\n",
    "#y_train[cols] = y_train[cols].applymap(np.int32)\n",
    "y_val[val_cols] = y_val[val_cols].astype(int)\n",
    "y_val = y_val.values\n",
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train.iloc[:,train_no_text.shape[1]:].head()\n",
    "x_train.iloc[:,:train_no_text.shape[1]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_cnn = x_train.iloc[:,train_no_text.shape[1]:]\n",
    "x_train_cnn = x_train_cnn.values\n",
    "\n",
    "x_val_cnn = x_val.iloc[:,train_no_text.shape[1]:]\n",
    "x_val_cnn = x_val_cnn.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_fc = x_train.iloc[:,:train_no_text.shape[1]]\n",
    "x_train_fc = x_train_fc.drop(['ID'], axis=1)\n",
    "x_train_fc = x_train_fc.values\n",
    "\n",
    "x_val_fc = x_val.iloc[:,:train_no_text.shape[1]]\n",
    "x_val_fc = x_val_fc.drop(['ID'], axis=1)\n",
    "x_val_fc = x_val_fc.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_fc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_cnn = test_MLP_CNN.iloc[:,test_full_no_text.shape[1]:]\n",
    "test_cnn = test_cnn.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fc = test_MLP_CNN.iloc[:,:test_full_no_text.shape[1]]\n",
    "test_fc = test_fc.drop(['ID'], axis=1)\n",
    "test_fc = test_fc.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.where(test_fc[4] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stage2_cnn = stage2_MLP_CNN.iloc[:,stage2_no_text.shape[1]:]\n",
    "stage2_cnn = stage2_cnn.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stage2_fc = stage2_MLP_CNN.iloc[:,:stage2_no_text.shape[1]]\n",
    "stage2_fc = stage2_fc.drop(['ID'], axis=1)\n",
    "stage2_fc = stage2_fc.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(WORD_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Glove trained embedding\n",
    "embedding_layer = Embedding(input_dim=len(WORD_INDEX) + 1,\n",
    "                            output_dim=EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=SEQ_LEN,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(SEQ_LEN,), dtype='int32', name='main_input')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "conv = Conv1D(64, 3, activation='relu')(embedded_sequences)\n",
    "conv = MaxPooling1D(5)(conv) #3 #add dropout after this\n",
    "conv = BatchNormalization()(conv)\n",
    "conv = Conv1D(128, 3, activation='relu')(conv)\n",
    "conv = MaxPooling1D(5)(conv)\n",
    "conv = Conv1D(256, 3, activation='relu')(conv) #remove\n",
    "conv = MaxPooling1D(35)(conv) #remove\n",
    "conv = Dropout(0.7)(conv)\n",
    "conv = Flatten()(conv)\n",
    "conv = BatchNormalization()(conv)\n",
    "conv = Dropout(0.7)(conv)\n",
    "conv = Dense(256, activation='relu')(conv)\n",
    "conv = BatchNormalization()(conv)\n",
    "conv = Dropout(0.7)(conv)\n",
    "conv_out = Dense(9, activation='softmax')(conv)\n",
    "\n",
    "#cnn = Model(inputs=sequence_input, outputs=conv_predictions)\n",
    "#cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FC_SEQ_LEN = int(x_train_fc.shape[1])\n",
    "FC_SEQ_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auxiliary_input = Input(shape=(FC_SEQ_LEN,), name='aux_input')\n",
    "\n",
    "x = keras.layers.concatenate([conv_out, auxiliary_input])\n",
    "\n",
    "# We stack a deep densely-connected network on top\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "# And finally we add the main logistic regression layer\n",
    "main_output = Dense(9, activation='softmax', name='main_output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[sequence_input, auxiliary_input], outputs=[main_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              loss_weights={'main_output': 1.0},\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#train initial model with 5 epochs\n",
    "#train model second time (with test predictions) with 4 epochs\n",
    "####################\n",
    "\n",
    "model.fit({'main_input': x_train_cnn, 'aux_input': x_train_fc}, {'main_output': y_train}, \n",
    "          validation_data=({'main_input': x_val_cnn, 'aux_input': x_val_fc}, {'main_output': y_val}),\n",
    "          epochs=4, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_preds = model.predict({'main_input': test_cnn, 'aux_input': test_fc}, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.argmax(test_preds, axis=1)\n",
    "test_labels = pd.DataFrame(test_labels, columns = ['Class'])\n",
    "test_labels['Class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_labels.append(pd.DataFrame([2,7,8], columns = ['Class']))\n",
    "test_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_one_hot = keras.utils.to_categorical(test_labels['Class'])\n",
    "test_labels_one_hot = pd.DataFrame(test_labels_one_hot)\n",
    "test_labels_one_hot.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_one_hot = test_labels_one_hot.drop(test_labels_one_hot.index[[5300,5301,5302]])\n",
    "test_labels_one_hot.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_arr = np.array(test_labels_one_hot.astype(int))\n",
    "test_labels_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(test_labels['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_arr[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_cnn = np.vstack((x_train_cnn,test_cnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_fc = np.vstack((x_train_fc,test_fc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np.vstack((y_train, test_labels_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stage2_preds_4epochs = model.predict({'main_input': stage2_cnn, 'aux_input': stage2_fc}, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_5epochs = pd.DataFrame(stage2_preds_5epochs)\n",
    "csv_5epochs.to_csv('stage2_preds_5epochs.csv', sep=',')\n",
    "csv_5epochs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stage2_5eps_labels = np.argmax(np.asarray(csv_5epochs), axis=1)\n",
    "stage2_5eps_labels = pd.DataFrame(stage2_5eps_labels, columns = ['Class'])\n",
    "stage2_5eps_labels['Class'] = stage2_5eps_labels['Class'].astype(str)\n",
    "#test_labels['Class'].unique()\n",
    "stage2_5eps_labels = pd.get_dummies(stage2_5eps_labels)\n",
    "stage2_5eps_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stage2_5eps_labels.to_csv('stage2_5eps_labels.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_4epochs = pd.DataFrame(stage2_preds_4epochs)\n",
    "csv_4epochs.to_csv('stage2_preds_4epochs.csv', sep=',')\n",
    "csv_4epochs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_4ep_labels = np.argmax(np.asarray(csv_4epochs), axis=1)\n",
    "stage2_4ep_labels = pd.DataFrame(stage2_4ep_labels, columns = ['Class'])\n",
    "stage2_4ep_labels['Class'] = stage2_4ep_labels['Class'].astype(str)\n",
    "#test_labels['Class'].unique()\n",
    "stage2_4ep_labels = pd.get_dummies(stage2_4ep_labels)\n",
    "stage2_4ep_labels.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
