{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theano import function, config, shared, tensor\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core\n",
    "iters = 1000\n",
    "\n",
    "rng = numpy.random.RandomState(22)\n",
    "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))\n",
    "f = function([], tensor.exp(x))\n",
    "print(f.maker.fgraph.toposort())\n",
    "t0 = time.time()\n",
    "for i in range(iters):\n",
    "    r = f()\n",
    "t1 = time.time()\n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\n",
    "print(\"Result is %s\" % (r,))\n",
    "if numpy.any([isinstance(x.op, tensor.Elemwise) and\n",
    "              ('Gpu' not in type(x.op).__name__)\n",
    "              for x in f.maker.fgraph.toposort()]):\n",
    "    print('Used the cpu')\n",
    "else:\n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy import stats, integrate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "import keras\n",
    "#from keras import backend as K\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import Nadam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST_TEXT_DIR = 'C:\\\\Users\\\\Chris\\\\Documents\\\\kaggle\\\\personalized medicine\\\\test_text\\\\'\n",
    "TEST_VARIANTS_DIR = 'C:\\\\Users\\\\Chris\\\\Documents\\\\kaggle\\\\personalized medicine\\\\test_variants\\\\'\n",
    "STAGE1_SOLUTIONS_DIR = 'C:\\\\Users\\\\Chris\\\\Documents\\\\kaggle\\\\personalized medicine\\\\'\n",
    "\n",
    "TRAINING_TEXT_DIR = 'C:\\\\Users\\\\Chris\\\\Documents\\\\kaggle\\\\personalized medicine\\\\training_text\\\\'\n",
    "TRAINING_VARIANTS_DIR = 'C:\\\\Users\\\\Chris\\\\Documents\\\\kaggle\\\\personalized medicine\\\\training_variants\\\\'\n",
    "\n",
    "STAGE2_TEXT_DIR = 'C:\\\\Users\\\\Chris\\\\Documents\\\\kaggle\\\\personalized medicine\\\\'\n",
    "STAGE2_VARIANTS_DIR = 'C:\\\\Users\\\\Chris\\\\Documents\\\\kaggle\\\\personalized medicine\\\\'\n",
    "\n",
    "GLOVE_DIR = 'C:\\\\Users\\\\Chris\\\\Documents\\\\kaggle\\\\personalized medicine\\\\glove.6B\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = pd.read_csv(os.path.join(TEST_TEXT_DIR, 'test_text'), sep = '\\n')\n",
    "test_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_text = test_text['ID,Text'].str.split('\\|\\|', expand=True, n=1)\n",
    "test_text = test_text.rename(columns={0: \"ID\", 1: \"Text\"})\n",
    "test_text['ID'] = test_text['ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text.tail()\n",
    "#test_text.shape\n",
    "type(test_text['ID'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_variants = pd.read_csv(os.path.join(TEST_VARIANTS_DIR, 'test_variants'))\n",
    "#train_vars = train_vars.sort_index(axis=1)\n",
    "test_variants['Group'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_variants.tail()\n",
    "#test_variants.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_solutions = pd.read_csv(os.path.join(STAGE1_SOLUTIONS_DIR, 'stage1_solution_filtered.csv'))\n",
    "stage1_solutions.head()\n",
    "#stage1_preclasses = stage1_solutions.drop('ID', axis=1).as_matrix()\n",
    "#stage1_preclasses\n",
    "# = pd.DataFrame(stage1_preclasses)\n",
    "#stage1_preclasses_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text = pd.read_csv(os.path.join(TRAINING_TEXT_DIR, 'training_text'), sep = '\\n')\n",
    "training_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_text = training_text['ID,Text'].str.split('\\|\\|', expand=True, n=1)\n",
    "training_text = training_text.rename(columns={0: \"ID\", 1: \"Text\"})\n",
    "training_text['ID'] = training_text['ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text.head()\n",
    "#training_text.shape\n",
    "type(training_text['ID'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_variants = pd.read_csv(os.path.join(TRAINING_VARIANTS_DIR, 'training_variants'))\n",
    "training_variants['Group'] = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_variants.head()\n",
    "#training_variants.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stage2 here to use variants for one hot encoding\n",
    "stage2_text = pd.read_csv(os.path.join(STAGE2_TEXT_DIR, 'stage2_test_text.csv'), sep = '\\n')\n",
    "stage2_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stage2_text = stage2_text['ID,Text'].str.split('\\|\\|', expand=True, n=1)\n",
    "stage2_text = stage2_text.rename(columns={0: \"ID\", 1: \"Text\"})\n",
    "stage2_text['ID'] = stage2_text['ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_text.tail()\n",
    "stage2_text.shape\n",
    "type(stage2_text['ID'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stage2_variants = pd.read_csv(os.path.join(STAGE2_VARIANTS_DIR, 'stage2_test_variants.csv'))\n",
    "stage2_variants['Group'] = 'stage2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_variants.tail()\n",
    "#test_variants.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants=pd.concat([training_variants.drop('Class', axis=1), test_variants, stage2_variants])\n",
    "variants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_dummies = pd.get_dummies(variants, columns=['Gene', 'Variation'], drop_first=True)\n",
    "variant_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_variants_dummies = variant_dummies[variant_dummies['Group'] == 'test']\n",
    "test_variants_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_no_labels = test_variants_dummies.merge(test_text, left_on='ID', right_on='ID')\n",
    "test_union = pd.merge(test_no_labels, stage1_solutions, on='ID', how='left')\n",
    "test_union.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full = test_union[test_union.isnull().any(axis=1)]\n",
    "test_full = test_full.reset_index(drop=True)\n",
    "test_full = test_full.iloc[:, :test_full.shape[1]-9]\n",
    "test_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_validations = test_union.dropna().copy()\n",
    "test_validations = test_validations.reset_index(drop=True)\n",
    "test_validations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_validations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_variants_dummies = variant_dummies[variant_dummies['Group'] == 'train']\n",
    "training_variants_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_dummies_text = training_variants_dummies.merge(training_text, left_on='ID', right_on='ID')\n",
    "training_dummies_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_variants['Class'] = training_variants['Class']-1\n",
    "training_variants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_one_hot = keras.utils.to_categorical(training_variants['Class'])\n",
    "training_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_one_hot_colnames = pd.DataFrame(training_one_hot, columns=['Class_0', 'Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6', 'Class_7', 'Class_8'])\n",
    "training_one_hot_colnames.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_one_hot_colnames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_full = pd.concat([training_dummies_text, training_one_hot_colnames], axis=1)\n",
    "training_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_full.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_variants_dummies = variant_dummies[variant_dummies['Group'] == 'stage2']\n",
    "stage2_variants_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_full = stage2_variants_dummies.merge(stage2_text, left_on='ID', right_on='ID')\n",
    "stage2_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_full.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_input_text(x):\n",
    "    x['Text'] = x['Text'].str.replace(r'(Go to: [0-9]. )(?!introduction/?|background).*', r'\\1', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'(Go to:) (?!introduction/?|background).*', r'\\1', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace('^.*?Go to: [0-9]. (introduction/?|background)', r'\\1', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace('^.*?Go to: (introduction/?|background)', r'\\1', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'^introduction', '', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'^background', '', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'Go to:', '')\n",
    "    x['Text'] = x['Text'].str.replace('^.*?(Key Words:)', r'\\1', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(' Results.*$| RESULTS .*$', '')\n",
    "    x['Text'] = x['Text'].str.replace('MATERIALS? AND.*$| Methodology.*$', '')\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x80\\x98', '\\'', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x80\\x99', '\\'', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x80\\x9a', ',', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x80\\x9c', '\"', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x80\\x9d', '\"', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\"', '', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x80\\xa2', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x80\\x93', '-', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x80\\x94', '-', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x88\\xbc', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x80\\x8a', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x80\\x82', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x80\\x83', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x82\\xac', '', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xc2\\xae', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xe2\\x88\\x92', '-', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xc3\\x82', '', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xc2\\xa1', '', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xc3\\xa2', '', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xc2\\xa9', '', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xce\\xb1', 'alpha', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xce\\xb2', 'beta', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xce\\xb3', 'gamma', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xce\\xb4', 'delta', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xce\\xb5', 'epsilon', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\xce\\xba', 'kappa', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\t[0-9]{0,3}\\/[0-9]{0,3}', '', flags=re.IGNORECASE) #reference\n",
    "    x['Text'] = x['Text'].str.replace(r'\\t\\[.*?\\]', '', flags=re.IGNORECASE) #reference\n",
    "    x['Text'] = x['Text'].str.replace(r'\\((\\d+, ?)+(\\d+)?\\)', '', flags=re.IGNORECASE) #comma sep ref list in rd brackets\n",
    "    x['Text'] = x['Text'].str.replace(r'\\(\\d+\\)', '', flags=re.IGNORECASE) #single reference in rd brack\n",
    "    x['Text'] = x['Text'].str.replace(r'\\(\\d+-\\d+\\)', '', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\[\\d+\\]', '', flags=re.IGNORECASE) #reference ref in sq brack\n",
    "    x['Text'] = x['Text'].str.replace(r'\\[\\d+-\\d+\\]', '', flags=re.IGNORECASE) #reference x-y in sq brack\n",
    "    x['Text'] = x['Text'].str.replace(r'\\[(\\d+, ?)+(\\d+)?\\]', '', flags=re.IGNORECASE) #comma sep ref list in sq brackets\n",
    "    x['Text'] = x['Text'].str.replace(r'\\(\\w+? et al.*?\\)', '', flags=re.IGNORECASE) #et el.\n",
    "    x['Text'] = x['Text'].str.replace(r'([A-Za-z]{6,}?)[0-9]+?(\\.)', r'\\1\\2', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'([A-Za-z]{6,}?)[0-9]+?(,)', r'\\1\\2', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'([A-Za-z]{6,}?)(\\d+, ?)+(\\d+)?(\\.)', r'\\1\\4', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'(\\d{1,3}?, ?)+(\\d+)?(\\.)', r'\\3', flags=re.IGNORECASE) #open csl\n",
    "    x['Text'] = x['Text'].str.replace(r'\\)(\\d{1,3}?)(\\.)', r'\\2', flags=re.IGNORECASE) #parentheses followed by 1 ref preiod\n",
    "    x['Text'] = x['Text'].str.replace(r'@\\.?EGFR', r'deltaEGFR', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\(to [A-Z]\\. [A-Z]\\.\\)', r'', flags=re.IGNORECASE) #funding abr\n",
    "    x['Text'] = x['Text'].str.replace(r' \\d{4,}\\.? ', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\b[tcga]+\\b', ' ')\n",
    "    x['Text'] = x['Text'].str.replace(r'\\(Fig\\.? ?\\d+ ?[A-Za-z]?.{0,4}\\)', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\(Figure\\.? ?\\d+ ?[A-Za-z]?.{0,4}\\)', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\d{1,2}\\/\\d{1,2}\\/\\d{4}', ' ')\n",
    "    x['Text'] = x['Text'].str.replace(r'\\d{1,2}\\/\\d{1,2}\\/\\d{2}', ' ')\n",
    "    x['Text'] = x['Text'].str.replace(r'((\\(\\d{3}\\) ?)|(\\d{3}-))?\\d{3}-\\d{4}', ' ', flags=re.IGNORECASE) #phone\n",
    "    x['Text'] = x['Text'].str.replace(r'\\(?(http|ftp|https):\\/\\/[\\w\\-_]+(\\.[\\w\\-_]+)+\\/?\\)?', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'\\(?www\\.[\\w\\-_]+(\\.[\\w\\-_]+)+\\/?\\)?', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'https?:\\/\\/(dx\\.)?doi\\.org\\/.*? ', '', flags=re.IGNORECASE) #http://dx?doi.org\n",
    "    x['Text'] = x['Text'].str.replace(r'doi: ?\\d+\\.\\d+\\/.*? ', '', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'Dept\\.?|Department of \\w+', ' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'Phone:', r' ', flags=re.IGNORECASE)\n",
    "    x['Text'] = x['Text'].str.replace(r'Fax:', r' ', flags=re.IGNORECASE)\n",
    "    return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt_train = process_input_text(training_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_train[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(tt_train.shape[0]):\n",
    "    tt_train.loc[i,'Text'] = tt_train.loc[i,'Text'].decode(\"utf-8\").encode(\"ascii\",\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt_test = process_input_text(test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(tt_test.shape[0]):\n",
    "    tt_test.loc[i,'Text'] = tt_test.loc[i,'Text'].decode(\"utf-8\").encode(\"ascii\",\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt_test_valids = process_input_text(test_validations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_test_valids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(tt_test_valids.shape[0]):\n",
    "    tt_test_valids.loc[i,'Text'] = tt_test_valids.loc[i,'Text'].decode(\"utf-8\").encode(\"ascii\",\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt_stage2 = process_input_text(stage2_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_stage2[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_stage2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(tt_stage2.shape[0]):\n",
    "    tt_stage2.loc[i,'Text'] = tt_stage2.loc[i,'Text'].decode(\"utf-8\").encode(\"ascii\",\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_stage2['Text'][150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tt_test['Text'].str.contains('\\\\\\\\x')\n",
    "tt_test.loc[150,'Text']\n",
    "#asciidata\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_test.loc[741]\n",
    "tt_test.loc[741, 'Text'][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#does entry lack \"reintroduction\n",
    "#reint_mask = ~tt['Text'].str.contains('reintroduction', flags=re.IGNORECASE)\n",
    "#reint_mask[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#theint_mask = ~tt['Text'].str.contains('the introduction', flags=re.IGNORECASE)\n",
    "#theint_mask[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#anint_mask = ~tt['Text'].str.contains('an introduction', flags=re.IGNORECASE)\n",
    "#anint_mask[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#theint_mask[742]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#leng = int(reint_mask.shape[0])\n",
    "#for i in range(leng):\n",
    "#    if (reint_mask[i] == True or theint_mask[i] == True or anint_mask[i] == True):\n",
    "#        tt['Text'][i] = re.sub('Introduction.{,20}?\\\\xe2\\\\x80\\\\xa2.*$', '', tt['Text'][i], flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#leng = int(reint_mask.shape[0])\n",
    "#for i in range(leng):\n",
    "#    if (reint_mask[i] == True or theint_mask[i] == True or anint_mask[i] == True):\n",
    "#        tt['Text'][i] = re.sub('^.*?Introduction', '', tt['Text'][i], flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tt.where(df_mask, tt['Text'].str.replace('Introduction.{,20}?\\\\xe2\\\\x80\\\\xa2.*$', ''))\n",
    "#tt['Text'].head()\n",
    "#tt['Text'] = tt['Text'].str.replace('Introduction.{,20}?\\\\xe2\\\\x80\\\\xa2.*$', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4808 characters - must convert to words for vocab length\n",
    "len(tt_test.loc[741, 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_test.loc[3227]\n",
    "tt_test.loc[3227, 'Text'][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_test.loc[923, 'Text']\n",
    "#tt_test.loc[923, 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_test.loc[150]\n",
    "tt_test.loc[150, 'Text']\n",
    "#1613\n",
    "#529\n",
    "#150\n",
    "#2364\n",
    "#2395"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(tt_stage2['Text'].str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "###########################\n",
    "#RE test here\n",
    "#\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_stage2['Text'].str.len().sort_values(ascending = True)[:20]\n",
    "#[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = pd.DataFrame(tt_test['Text'].str.len())\n",
    "#lens.head()\n",
    "#lens.idxmax()\n",
    "lens.median()\n",
    "#lens.quantile(q=0.75)\n",
    "#If needed, just take the first 5000 words in each entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_corpus = tt_train['Text'].values.tolist()\n",
    "test_corpus = tt_test['Text'].values.tolist()\n",
    "stage2_corpus = tt_stage2['Text'].values.tolist()\n",
    "train_test_stage2_corpus = train_corpus + test_corpus + stage2_corpus\n",
    "\n",
    "test_val_corpus = tt_test_valids['Text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_stage2_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IN GENERAL, DONT MAKE OWN VOCAB\n",
    "#DOWNLAOD GLOVE OR WORD2VEC\n",
    "SEQ_LEN = 2000\n",
    "MAX_WORDS = 5000\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "\n",
    "#or take all words as tokens\n",
    "#tokenizermax_words = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probably have to use a loop for this\n",
    "tokenizer.fit_on_texts(train_test_stage2_corpus)\n",
    "WORD_INDEX = tokenizer.word_index\n",
    "print(WORD_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted(WORD_INDEX.items(), key=operator.itemgetter(1))\n",
    "#sorted(WORD_INDEX.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Found %s unique tokens.' % len(WORD_INDEX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_seq = tokenizer.texts_to_sequences(train_corpus)\n",
    "test_seq = tokenizer.texts_to_sequences(test_corpus)\n",
    "test_val_seq = tokenizer.texts_to_sequences(test_val_corpus)\n",
    "stage2_seq = tokenizer.texts_to_sequences(stage2_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_val_seq[0] #1277:1279"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def limit_words(x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i][:SEQ_LEN]\n",
    "    return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_seq_limit = limit_words(train_seq)\n",
    "test_seq_limit = limit_words(test_seq)\n",
    "test_val_seq_limit = limit_words(test_val_seq)\n",
    "stage2_seq_limit = limit_words(stage2_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_seq[1278])\n",
    "len(test_seq_limit[1278])\n",
    "#test_seq_limit[1278]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pad_sequences(train_seq_limit, maxlen=SEQ_LEN, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pad_sequences(test_seq_limit, maxlen=SEQ_LEN, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_val_data = pad_sequences(test_val_seq, maxlen=SEQ_LEN, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stage2_data = pad_sequences(stage2_seq_limit, maxlen=SEQ_LEN, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(WORD_INDEX) + 1, EMBEDDING_DIM))\n",
    "for word, i in WORD_INDEX.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################3\n",
    "#######################################333\n",
    "########################################3\n",
    "#######################################\n",
    "\n",
    "#Need to combine training_full and test full genes and vars before one hot encoding - for mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Come back to this if needed\n",
    "#pre_MLP_CNN_variants = pd.concat([train_no_text, pd.DataFrame(train_data), pd.DataFrame(train_labels)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchfor = ['ID', 'Gene', 'Variation']\n",
    "searchfor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_text = training_full.loc[:,training_full.columns.str.contains('|'.join(searchfor))]\n",
    "train_no_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = training_full.loc[:,training_full.columns.str.contains('Class')]\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MLP_CNN = pd.concat([train_no_text, pd.DataFrame(train_data), train_labels], axis=1)\n",
    "train_MLP_CNN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_split = .iloc[:,train_no_text.shape[1]:]\n",
    "#x_split = train_MLP_CNN.drop(['Class'], axis=1)\n",
    "x_split = train_MLP_CNN.iloc[:,:(train_MLP_CNN.shape[1]-9)]\n",
    "y_split = train_MLP_CNN.iloc[:,(train_MLP_CNN.shape[1]-9):]\n",
    "x_split.head()\n",
    "#y_split.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_val_no_text = test_validations.loc[:,test_validations.columns.str.contains('|'.join(searchfor))]\n",
    "test_val_no_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_val_no_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vals_labels = test_validations.iloc[:,(test_validations.shape[1]-9):]\n",
    "test_vals_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_MLP_CNN = pd.concat([test_val_no_text, pd.DataFrame(test_val_data), test_vals_labels], axis=1)\n",
    "val_MLP_CNN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_MLP_CNN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validation = val_MLP_CNN.iloc[:,:(val_MLP_CNN.shape[1]-9)]\n",
    "y_validation = val_MLP_CNN.iloc[:,(val_MLP_CNN.shape[1]-9):]\n",
    "x_validation.head()\n",
    "#y_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full_no_text = test_full.loc[:,test_full.columns.str.contains('|'.join(searchfor))]\n",
    "test_full_no_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full_no_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_MLP_CNN = pd.concat([test_full_no_text, pd.DataFrame(test_data)], axis=1)\n",
    "test_MLP_CNN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_MLP_CNN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_no_text = stage2_full.loc[:,stage2_full.columns.str.contains('|'.join(searchfor))]\n",
    "stage2_no_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_no_text.head()\n",
    "stage2_no_text.shape[1]\n",
    "stage2_no_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_MLP_CNN = pd.concat([stage2_no_text, pd.DataFrame(stage2_data)], axis=1)\n",
    "val_MLP_CNN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_split, y_split, stratify=y_split, train_size=0.90, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = pd.concat([x_train, x_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_val = pd.concat([x_val, x_validation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.concat([y_train, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = y_train.columns.values.tolist()\n",
    "cols\n",
    "#y_train[cols] = y_train[cols].applymap(np.int32)\n",
    "y_train[cols] = y_train[cols].astype(int)\n",
    "y_train = y_train.values\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_val = pd.concat([y_val, y_validation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cols = y_val.columns.values.tolist()\n",
    "val_cols\n",
    "#y_train[cols] = y_train[cols].applymap(np.int32)\n",
    "y_val[val_cols] = y_val[val_cols].astype(int)\n",
    "y_val = y_val.values\n",
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train.iloc[:,train_no_text.shape[1]:].head()\n",
    "x_train.iloc[:,:train_no_text.shape[1]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_cnn = x_train.iloc[:,train_no_text.shape[1]:]\n",
    "x_train_cnn = x_train_cnn.values\n",
    "\n",
    "x_val_cnn = x_val.iloc[:,train_no_text.shape[1]:]\n",
    "x_val_cnn = x_val_cnn.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_fc = x_train.iloc[:,:train_no_text.shape[1]]\n",
    "x_train_fc = x_train_fc.drop(['ID'], axis=1)\n",
    "x_train_fc = x_train_fc.values\n",
    "\n",
    "x_val_fc = x_val.iloc[:,:train_no_text.shape[1]]\n",
    "x_val_fc = x_val_fc.drop(['ID'], axis=1)\n",
    "x_val_fc = x_val_fc.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_fc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_cnn = test_MLP_CNN.iloc[:,test_full_no_text.shape[1]:]\n",
    "test_cnn = test_cnn.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fc = test_MLP_CNN.iloc[:,:test_full_no_text.shape[1]]\n",
    "test_fc = test_fc.drop(['ID'], axis=1)\n",
    "test_fc = test_fc.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_fc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stage2_cnn = stage2_MLP_CNN.iloc[:,stage2_no_text.shape[1]:]\n",
    "stage2_cnn = stage2_cnn.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_cnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_fc = stage2_MLP_CNN.iloc[:,:stage2_no_text.shape[1]]\n",
    "stage2_fc = stage2_fc.drop(['ID'], axis=1)\n",
    "stage2_fc = stage2_fc.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_fc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(WORD_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Glove trained embedding\n",
    "embedding_layer = Embedding(input_dim=len(WORD_INDEX) + 1,\n",
    "                            output_dim=EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=SEQ_LEN,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(SEQ_LEN,), dtype='int32', name='main_input')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "conv = Conv1D(64, 3, activation='relu')(embedded_sequences)\n",
    "conv = MaxPooling1D(3)(conv)\n",
    "conv = Dropout(0.7)(conv)\n",
    "conv = BatchNormalization()(conv)\n",
    "conv = Conv1D(128, 3, activation='relu')(conv)\n",
    "conv = MaxPooling1D(5)(conv)\n",
    "conv = Flatten()(conv)\n",
    "conv = BatchNormalization()(conv)\n",
    "conv = Dropout(0.7)(conv)\n",
    "conv = Dense(128, activation='relu')(conv)\n",
    "conv_out = Dense(9, activation='softmax')(conv)\n",
    "\n",
    "#cnn = Model(inputs=sequence_input, outputs=conv_predictions)\n",
    "#cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FC_SEQ_LEN = int(x_train_fc.shape[1])\n",
    "FC_SEQ_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auxiliary_input = Input(shape=(FC_SEQ_LEN,), name='aux_input')\n",
    "\n",
    "x = keras.layers.concatenate([conv_out, auxiliary_input])\n",
    "\n",
    "# We stack a deep densely-connected network on top\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "# And finally we add the main logistic regression layer\n",
    "main_output = Dense(9, activation='softmax', name='main_output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[sequence_input, auxiliary_input], outputs=[main_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              loss_weights={'main_output': 1.0},\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU 12 epochs\n",
    "# And trained it via:\n",
    "model.fit({'main_input': x_train_cnn, 'aux_input': x_train_fc}, {'main_output': y_train}, \n",
    "          validation_data=({'main_input': x_val_cnn, 'aux_input': x_val_fc}, {'main_output': y_val}),\n",
    "          epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_preds = model.predict({'main_input': test_cnn, 'aux_input': test_fc}, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stage2_preds = model.predict({'main_input': stage2_cnn, 'aux_input': stage2_fc}, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv = pd.DataFrame(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv.to_csv('full_preds.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#HIDDEN_NODES = (FC_SEQ_LEN + 9)/2\n",
    "HIDDEN_NODES = 5000\n",
    "HIDDEN_NODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This returns a tensor\n",
    "fc_inputs = Input(shape=(FC_SEQ_LEN,))\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "fc = Dense(FC_SEQ_LEN, activation='relu')(fc_inputs)\n",
    "fc = Dense(HIDDEN_NODES, activation='relu')(fc)\n",
    "fc = Dropout(0.5)(fc)\n",
    "fc = BatchNormalization()(fc)\n",
    "#fc = Dense(HIDDEN_NODES, activation='relu')(fc)\n",
    "#fc = BatchNormalization()(fc)\n",
    "#fc = Dropout(0.5)(fc)\n",
    "#fc = Dense(HIDDEN_NODES, activation='relu')(fc)\n",
    "#fc = BatchNormalization()(fc)\n",
    "#fc = Dropout(0.5)(fc)\n",
    "fc_predictions = Dense(9, activation='softmax')(fc)\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "mlp = Model(inputs=fc_inputs, outputs=fc_predictions)\n",
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.00001),\n",
    "              metrics=['acc'])\n",
    "\n",
    "mlp.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.00001),\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SZ = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#padding='valid'\n",
    "\n",
    "#def scheduler(epoch):\n",
    "#    if epoch == 5:\n",
    "#        model.lr.set_value(.02)\n",
    "#    return model.lr.get_value()\n",
    "\n",
    "#change_lr = LearningRateScheduler(scheduler)\n",
    "\n",
    "#model.fit(x_embed, y, nb_epoch=1, batch_size = batch_size, show_accuracy=True, callbacks=[chage_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnn.fit(x_train_cnn, y_train, validation_data=(x_val_cnn, y_val),\n",
    "          epochs=10, batch_size=BATCH_SZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_preds = cnn.predict(test_data, batch_size=BATCH_SZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp.fit(x_train_fc, y_train, validation_data=(x_val_fc, y_val),\n",
    "          epochs=10, batch_size=BATCH_SZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_preds = mlp.predict(test_fc, batch_size=BATCH_SZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####\n",
    "####Pseudo\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(test_data).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_preds_df = pd.DataFrame(test_preds, columns=['Class'])\n",
    "test_MLP_CNN = pd.concat([test_full[['ID', 'Gene', 'Variation']], pd.DataFrame(test_data), pd.DataFrame(test_preds_df)], axis=1)\n",
    "test_MLP_CNN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_FRAC=0.28\n",
    "\n",
    "grouped = test_MLP_CNN.groupby('Class')\n",
    "sampled = grouped.apply(lambda x: x.sample(frac=SAMPLE_FRAC))\n",
    "sampled = sampled.reset_index(drop=True)\n",
    "sampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pseudo = pd.concat([train_MLP_CNN, sampled], ignore_index=True)\n",
    "pseudo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
